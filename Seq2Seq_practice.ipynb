{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Seq2Seq.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "mount_file_id": "1CiPJU5PXOeqy4SCddHQa4diG0pvMF_s3",
      "authorship_tag": "ABX9TyMb8np6LmPL7evPppRSarsH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yoo9519-AIdev/Learning/blob/master/Seq2Seq_practice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IkG5wcgKdIy2",
        "colab_type": "text"
      },
      "source": [
        "## 병렬 코퍼스와 전처리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2qdIk_YSUlU3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import urllib3\n",
        "import zipfile\n",
        "import shutil\n",
        "import os\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKkM79siXfxh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "http = urllib3.PoolManager()\n",
        "url ='http://www.manythings.org/anki/fra-eng.zip'\n",
        "filename = 'fra-eng.zip'\n",
        "path = os.getcwd()\n",
        "zipfilename = os.path.join(path, filename)\n",
        "with http.request('GET', url, preload_content=False) as r, open(zipfilename, 'wb') as out_file:       \n",
        "    shutil.copyfileobj(r, out_file)\n",
        "\n",
        "with zipfile.ZipFile(zipfilename, 'r') as zip_ref:\n",
        "    zip_ref.extractall(path)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58TnCyA2X3S0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 32
        },
        "outputId": "315ec8cf-3111-4f13-f7e9-3f753eed0aa0"
      },
      "source": [
        "lines= pd.read_csv('fra.txt', names=['src', 'tar'], sep='\\t')\n",
        "len(lines)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "177210"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tra3RPPTX7IH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 352
        },
        "outputId": "7c35100d-f3c9-4e12-c651-76d1c8cb8733"
      },
      "source": [
        "lines = lines.loc[:, 'src':'tar']\n",
        "lines = lines[0:60000] # 데애터가 너무 많으므로 6만개만 사용\n",
        "lines.sample(10)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>src</th>\n",
              "      <th>tar</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Let's see what happens.</th>\n",
              "      <td>Voyons voir ce qui arrive.</td>\n",
              "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>That was embarrassing.</th>\n",
              "      <td>C'était embarrassant.</td>\n",
              "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>We'll protect you.</th>\n",
              "      <td>Nous te protégerons.</td>\n",
              "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>The donkey brayed.</th>\n",
              "      <td>L'âne brait.</td>\n",
              "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #7...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>I feel so lonely.</th>\n",
              "      <td>Je me sens si seule.</td>\n",
              "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Do you drink?</th>\n",
              "      <td>Buvez-vous ?</td>\n",
              "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>He's active and fit.</th>\n",
              "      <td>Il est actif et en forme.</td>\n",
              "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>What is that?</th>\n",
              "      <td>Qu'est-ce ?</td>\n",
              "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #4...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Watch closely.</th>\n",
              "      <td>Regardez de près !</td>\n",
              "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Don't eat without me.</th>\n",
              "      <td>Ne mange pas sans moi !</td>\n",
              "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                src                                                tar\n",
              "Let's see what happens.  Voyons voir ce qui arrive.  CC-BY 2.0 (France) Attribution: tatoeba.org #1...\n",
              "That was embarrassing.        C'était embarrassant.  CC-BY 2.0 (France) Attribution: tatoeba.org #2...\n",
              "We'll protect you.             Nous te protégerons.  CC-BY 2.0 (France) Attribution: tatoeba.org #2...\n",
              "The donkey brayed.                     L'âne brait.  CC-BY 2.0 (France) Attribution: tatoeba.org #7...\n",
              "I feel so lonely.              Je me sens si seule.  CC-BY 2.0 (France) Attribution: tatoeba.org #2...\n",
              "Do you drink?                          Buvez-vous ?  CC-BY 2.0 (France) Attribution: tatoeba.org #2...\n",
              "He's active and fit.      Il est actif et en forme.  CC-BY 2.0 (France) Attribution: tatoeba.org #1...\n",
              "What is that?                           Qu'est-ce ?  CC-BY 2.0 (France) Attribution: tatoeba.org #4...\n",
              "Watch closely.                   Regardez de près !  CC-BY 2.0 (France) Attribution: tatoeba.org #2...\n",
              "Don't eat without me.       Ne mange pas sans moi !  CC-BY 2.0 (France) Attribution: tatoeba.org #2..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SV_Od2GsdYVU",
        "colab_type": "text"
      },
      "source": [
        "fra.txt 데이터는 왼쪽의 영어 문장과 오른쪽의 프랑스어 문장 사이에 탭으로 구분되는 구조가 하나의 샘플이다.  \n",
        "앞으로의 코드에서 src는 입력 문장을 나타내며, tar는 번역하고자 하는 문장을 나타낸다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SRZ9p9bLYCyI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 352
        },
        "outputId": "e0128101-7aab-4ea3-d80f-1baa4537e0d5"
      },
      "source": [
        "# 프랑스 데이터에는 시작을 의미하는 '\\t', 종료를 의미하는 '\\n'심볼을 넣어주어야 한다.\n",
        "\n",
        "lines.tar = lines.tar.apply(lambda x : '\\t '+ x + ' \\n')\n",
        "lines.sample(10)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>src</th>\n",
              "      <th>tar</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>He has enough money.</th>\n",
              "      <td>Il dispose de suffisamment d'argent.</td>\n",
              "      <td>\\t CC-BY 2.0 (France) Attribution: tatoeba.org...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>He's after me.</th>\n",
              "      <td>Il est après mes fesses.</td>\n",
              "      <td>\\t CC-BY 2.0 (France) Attribution: tatoeba.org...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>I'm faithful to my wife.</th>\n",
              "      <td>Je suis fidèle envers ma femme.</td>\n",
              "      <td>\\t CC-BY 2.0 (France) Attribution: tatoeba.org...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>We count everything.</th>\n",
              "      <td>Nous comptons tout.</td>\n",
              "      <td>\\t CC-BY 2.0 (France) Attribution: tatoeba.org...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Tom says he can do that.</th>\n",
              "      <td>Tom dit qu'il peut le faire.</td>\n",
              "      <td>\\t CC-BY 2.0 (France) Attribution: tatoeba.org...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>I slept very well.</th>\n",
              "      <td>J'ai très bien dormi.</td>\n",
              "      <td>\\t CC-BY 2.0 (France) Attribution: tatoeba.org...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>I'm volunteering.</th>\n",
              "      <td>Je fais du bénévolat.</td>\n",
              "      <td>\\t CC-BY 2.0 (France) Attribution: tatoeba.org...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>I'll attend.</th>\n",
              "      <td>Je serai présent.</td>\n",
              "      <td>\\t CC-BY 2.0 (France) Attribution: tatoeba.org...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>He is well off now.</th>\n",
              "      <td>Il est riche, maintenant.</td>\n",
              "      <td>\\t CC-BY 2.0 (France) Attribution: tatoeba.org...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Who wants jello?</th>\n",
              "      <td>Qui veut de la gelée ?</td>\n",
              "      <td>\\t CC-BY 2.0 (France) Attribution: tatoeba.org...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                           src                                                tar\n",
              "He has enough money.      Il dispose de suffisamment d'argent.  \\t CC-BY 2.0 (France) Attribution: tatoeba.org...\n",
              "He's after me.                        Il est après mes fesses.  \\t CC-BY 2.0 (France) Attribution: tatoeba.org...\n",
              "I'm faithful to my wife.       Je suis fidèle envers ma femme.  \\t CC-BY 2.0 (France) Attribution: tatoeba.org...\n",
              "We count everything.                       Nous comptons tout.  \\t CC-BY 2.0 (France) Attribution: tatoeba.org...\n",
              "Tom says he can do that.          Tom dit qu'il peut le faire.  \\t CC-BY 2.0 (France) Attribution: tatoeba.org...\n",
              "I slept very well.                       J'ai très bien dormi.  \\t CC-BY 2.0 (France) Attribution: tatoeba.org...\n",
              "I'm volunteering.                        Je fais du bénévolat.  \\t CC-BY 2.0 (France) Attribution: tatoeba.org...\n",
              "I'll attend.                                 Je serai présent.  \\t CC-BY 2.0 (France) Attribution: tatoeba.org...\n",
              "He is well off now.                  Il est riche, maintenant.  \\t CC-BY 2.0 (France) Attribution: tatoeba.org...\n",
              "Who wants jello?                        Qui veut de la gelée ?  \\t CC-BY 2.0 (France) Attribution: tatoeba.org..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OEVvIWm_Ycwn",
        "colab_type": "text"
      },
      "source": [
        "글자 집합을 생성  \n",
        "글자 집합을 생성하는 이유는 토큰 단위가 글자이기 때문"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m6JM6B1fdxJI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# "
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gU8Jy2DVZb01",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 글자 집합 구축\n",
        "\n",
        "src_vocab=set()\n",
        "for line in lines.src: # 1줄씩 읽음\n",
        "    for char in line: # 1개의 글자씩 읽음\n",
        "        src_vocab.add(char)\n",
        "\n",
        "tar_vocab=set()\n",
        "for line in lines.tar:\n",
        "    for char in line:\n",
        "        tar_vocab.add(char)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xg9OibI_Zh7m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49
        },
        "outputId": "a8d798b7-f416-47a6-d00d-6aec84dc06d5"
      },
      "source": [
        "# 글자 집합의 크기\n",
        "\n",
        "src_vocab_size = len(src_vocab)+1\n",
        "tar_vocab_size = len(tar_vocab)+1\n",
        "print(src_vocab_size)\n",
        "print(tar_vocab_size)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "103\n",
            "75\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YznBDpiKZkYV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49
        },
        "outputId": "a0234d12-3fb5-4bc3-f982-00e19822c702"
      },
      "source": [
        "# 글자 집합에 글자 단위로 저장 된 것을 확인\n",
        "\n",
        "src_vocab = sorted(list(src_vocab))\n",
        "tar_vocab = sorted(list(tar_vocab))\n",
        "print(src_vocab[45:75])\n",
        "print(tar_vocab[45:75])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
            "['Z', '\\\\', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OkVLsoBGZrqS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "6594ed64-450d-4067-b40f-30d12cdf6404"
      },
      "source": [
        "# 이제 각 글자에 인덱스를 부여\n",
        "\n",
        "src_to_index = dict([(word, i+1) for i, word in enumerate(src_vocab)])\n",
        "tar_to_index = dict([(word, i+1) for i, word in enumerate(tar_vocab)])\n",
        "print(src_to_index)\n",
        "print(tar_to_index)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{' ': 1, '!': 2, '\"': 3, '%': 4, '&': 5, \"'\": 6, '(': 7, ')': 8, ',': 9, '-': 10, '.': 11, '0': 12, '1': 13, '2': 14, '3': 15, '4': 16, '5': 17, '6': 18, '7': 19, '8': 20, '9': 21, ':': 22, '?': 23, 'A': 24, 'B': 25, 'C': 26, 'D': 27, 'E': 28, 'F': 29, 'G': 30, 'H': 31, 'I': 32, 'J': 33, 'K': 34, 'L': 35, 'M': 36, 'N': 37, 'O': 38, 'P': 39, 'Q': 40, 'R': 41, 'S': 42, 'T': 43, 'U': 44, 'V': 45, 'W': 46, 'X': 47, 'Y': 48, 'Z': 49, 'a': 50, 'b': 51, 'c': 52, 'd': 53, 'e': 54, 'f': 55, 'g': 56, 'h': 57, 'i': 58, 'j': 59, 'k': 60, 'l': 61, 'm': 62, 'n': 63, 'o': 64, 'p': 65, 'q': 66, 'r': 67, 's': 68, 't': 69, 'u': 70, 'v': 71, 'w': 72, 'x': 73, 'y': 74, 'z': 75, '\\xa0': 76, '«': 77, '»': 78, 'À': 79, 'Ç': 80, 'É': 81, 'Ê': 82, 'Ô': 83, 'à': 84, 'â': 85, 'ç': 86, 'è': 87, 'é': 88, 'ê': 89, 'ë': 90, 'î': 91, 'ï': 92, 'ô': 93, 'ù': 94, 'û': 95, 'œ': 96, 'С': 97, '\\u2009': 98, '\\u200b': 99, '‘': 100, '’': 101, '\\u202f': 102}\n",
            "{'\\t': 1, '\\n': 2, ' ': 3, '#': 4, '&': 5, '(': 6, ')': 7, '-': 8, '.': 9, '0': 10, '1': 11, '2': 12, '3': 13, '4': 14, '5': 15, '6': 16, '7': 17, '8': 18, '9': 19, ':': 20, 'A': 21, 'B': 22, 'C': 23, 'D': 24, 'E': 25, 'F': 26, 'G': 27, 'H': 28, 'I': 29, 'J': 30, 'K': 31, 'L': 32, 'M': 33, 'N': 34, 'O': 35, 'P': 36, 'Q': 37, 'R': 38, 'S': 39, 'T': 40, 'U': 41, 'V': 42, 'W': 43, 'X': 44, 'Y': 45, 'Z': 46, '\\\\': 47, '_': 48, 'a': 49, 'b': 50, 'c': 51, 'd': 52, 'e': 53, 'f': 54, 'g': 55, 'h': 56, 'i': 57, 'j': 58, 'k': 59, 'l': 60, 'm': 61, 'n': 62, 'o': 63, 'p': 64, 'q': 65, 'r': 66, 's': 67, 't': 68, 'u': 69, 'v': 70, 'w': 71, 'x': 72, 'y': 73, 'z': 74}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ROf4Y1usZ0LV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 32
        },
        "outputId": "f1d6e9fb-b608-4244-c288-3aef71268612"
      },
      "source": [
        "# 인덱스가 부여된 글자 집합으로부터 갖고 있는 훈련 데이터에 대한 정수 인코딩 진행\n",
        "\n",
        "encoder_input = []\n",
        "for line in lines.src: #입력 데이터에서 1줄씩 문장을 읽음\n",
        "    temp_X = []\n",
        "    for w in line: #각 줄에서 1개씩 글자를 읽음\n",
        "      temp_X.append(src_to_index[w]) # 글자를 해당되는 정수로 변환\n",
        "    encoder_input.append(temp_X)\n",
        "print(encoder_input[:5])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[45, 50, 1, 2], [42, 50, 61, 70, 69, 1, 2], [42, 50, 61, 70, 69, 11], [26, 64, 70, 67, 68, 102, 2], [26, 64, 70, 67, 54, 75, 102, 2]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4L7dyuGhaBOo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "0a29b265-c732-403f-ae18-6ac9c38333ce"
      },
      "source": [
        "# 디코더에 입력값이 될 프랑스어 데이터에 대해서 정수 인코딩 진행\n",
        "\n",
        "decoder_input = []\n",
        "for line in lines.tar:\n",
        "    temp_X = []\n",
        "    for w in line:\n",
        "      temp_X.append(tar_to_index[w])\n",
        "    decoder_input.append(temp_X)\n",
        "print(decoder_input[:5])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1, 3, 23, 23, 8, 22, 45, 3, 12, 9, 10, 3, 6, 26, 66, 49, 62, 51, 53, 7, 3, 21, 68, 68, 66, 57, 50, 69, 68, 57, 63, 62, 20, 3, 68, 49, 68, 63, 53, 50, 49, 9, 63, 66, 55, 3, 4, 12, 18, 17, 17, 12, 17, 12, 3, 6, 23, 33, 7, 3, 5, 3, 4, 11, 11, 15, 18, 12, 15, 10, 3, 6, 43, 57, 68, 68, 73, 52, 53, 70, 7, 3, 2], [1, 3, 23, 23, 8, 22, 45, 3, 12, 9, 10, 3, 6, 26, 66, 49, 62, 51, 53, 7, 3, 21, 68, 68, 66, 57, 50, 69, 68, 57, 63, 62, 20, 3, 68, 49, 68, 63, 53, 50, 49, 9, 63, 66, 55, 3, 4, 15, 13, 18, 11, 12, 13, 3, 6, 23, 33, 7, 3, 5, 3, 4, 15, 10, 19, 18, 11, 19, 3, 6, 21, 57, 58, 57, 7, 3, 2], [1, 3, 23, 23, 8, 22, 45, 3, 12, 9, 10, 3, 6, 26, 66, 49, 62, 51, 53, 7, 3, 21, 68, 68, 66, 57, 50, 69, 68, 57, 63, 62, 20, 3, 68, 49, 68, 63, 53, 50, 49, 9, 63, 66, 55, 3, 4, 15, 13, 18, 11, 12, 13, 3, 6, 23, 33, 7, 3, 5, 3, 4, 14, 13, 12, 10, 14, 16, 12, 3, 6, 55, 57, 60, 60, 69, 72, 7, 3, 2], [1, 3, 23, 23, 8, 22, 45, 3, 12, 9, 10, 3, 6, 26, 66, 49, 62, 51, 53, 7, 3, 21, 68, 68, 66, 57, 50, 69, 68, 57, 63, 62, 20, 3, 68, 49, 68, 63, 53, 50, 49, 9, 63, 66, 55, 3, 4, 19, 10, 16, 13, 12, 18, 3, 6, 64, 49, 64, 49, 50, 53, 49, 66, 7, 3, 5, 3, 4, 19, 10, 16, 13, 13, 11, 3, 6, 67, 49, 51, 66, 53, 52, 51, 53, 60, 68, 57, 51, 7, 3, 2], [1, 3, 23, 23, 8, 22, 45, 3, 12, 9, 10, 3, 6, 26, 66, 49, 62, 51, 53, 7, 3, 21, 68, 68, 66, 57, 50, 69, 68, 57, 63, 62, 20, 3, 68, 49, 68, 63, 53, 50, 49, 9, 63, 66, 55, 3, 4, 19, 10, 16, 13, 12, 18, 3, 6, 64, 49, 64, 49, 50, 53, 49, 66, 7, 3, 5, 3, 4, 19, 10, 16, 13, 13, 12, 3, 6, 67, 49, 51, 66, 53, 52, 51, 53, 60, 68, 57, 51, 7, 3, 2]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Noy52mK1aQYU",
        "colab_type": "text"
      },
      "source": [
        "디코더의 예측값과 비교할 실체값이 필요  \n",
        "이 실체값에서는 '\\t'이 필요 없다.  \n",
        "(Dense, softmax 단어들 참고) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eXKBBmyJa7Dt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "0d885157-9fa6-43ac-8090-01152595740e"
      },
      "source": [
        "decoder_target = []\n",
        "for line in lines.tar:\n",
        "    t=0\n",
        "    temp_X = []\n",
        "    for w in line:\n",
        "      if t>0:\n",
        "        temp_X.append(tar_to_index[w])\n",
        "      t=t+1\n",
        "    decoder_target.append(temp_X)\n",
        "print(decoder_target[:5])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[3, 23, 23, 8, 22, 45, 3, 12, 9, 10, 3, 6, 26, 66, 49, 62, 51, 53, 7, 3, 21, 68, 68, 66, 57, 50, 69, 68, 57, 63, 62, 20, 3, 68, 49, 68, 63, 53, 50, 49, 9, 63, 66, 55, 3, 4, 12, 18, 17, 17, 12, 17, 12, 3, 6, 23, 33, 7, 3, 5, 3, 4, 11, 11, 15, 18, 12, 15, 10, 3, 6, 43, 57, 68, 68, 73, 52, 53, 70, 7, 3, 2], [3, 23, 23, 8, 22, 45, 3, 12, 9, 10, 3, 6, 26, 66, 49, 62, 51, 53, 7, 3, 21, 68, 68, 66, 57, 50, 69, 68, 57, 63, 62, 20, 3, 68, 49, 68, 63, 53, 50, 49, 9, 63, 66, 55, 3, 4, 15, 13, 18, 11, 12, 13, 3, 6, 23, 33, 7, 3, 5, 3, 4, 15, 10, 19, 18, 11, 19, 3, 6, 21, 57, 58, 57, 7, 3, 2], [3, 23, 23, 8, 22, 45, 3, 12, 9, 10, 3, 6, 26, 66, 49, 62, 51, 53, 7, 3, 21, 68, 68, 66, 57, 50, 69, 68, 57, 63, 62, 20, 3, 68, 49, 68, 63, 53, 50, 49, 9, 63, 66, 55, 3, 4, 15, 13, 18, 11, 12, 13, 3, 6, 23, 33, 7, 3, 5, 3, 4, 14, 13, 12, 10, 14, 16, 12, 3, 6, 55, 57, 60, 60, 69, 72, 7, 3, 2], [3, 23, 23, 8, 22, 45, 3, 12, 9, 10, 3, 6, 26, 66, 49, 62, 51, 53, 7, 3, 21, 68, 68, 66, 57, 50, 69, 68, 57, 63, 62, 20, 3, 68, 49, 68, 63, 53, 50, 49, 9, 63, 66, 55, 3, 4, 19, 10, 16, 13, 12, 18, 3, 6, 64, 49, 64, 49, 50, 53, 49, 66, 7, 3, 5, 3, 4, 19, 10, 16, 13, 13, 11, 3, 6, 67, 49, 51, 66, 53, 52, 51, 53, 60, 68, 57, 51, 7, 3, 2], [3, 23, 23, 8, 22, 45, 3, 12, 9, 10, 3, 6, 26, 66, 49, 62, 51, 53, 7, 3, 21, 68, 68, 66, 57, 50, 69, 68, 57, 63, 62, 20, 3, 68, 49, 68, 63, 53, 50, 49, 9, 63, 66, 55, 3, 4, 19, 10, 16, 13, 12, 18, 3, 6, 64, 49, 64, 49, 50, 53, 49, 66, 7, 3, 5, 3, 4, 19, 10, 16, 13, 13, 12, 3, 6, 67, 49, 51, 66, 53, 52, 51, 53, 60, 68, 57, 51, 7, 3, 2]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67Y8I2cfa_Qn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49
        },
        "outputId": "9c767024-e30f-47eb-dfb1-eba43a49d5cc"
      },
      "source": [
        "# 패딩 적용\n",
        "\n",
        "max_src_len = max([len(line) for line in lines.src])\n",
        "max_tar_len = max([len(line) for line in lines.tar])\n",
        "print(max_src_len)\n",
        "print(max_tar_len)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "72\n",
            "106\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MT0srFrrbrBu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 가장 긴 샘플의 길이에 맞춰서 패딩 적용\n",
        "\n",
        "encoder_input = pad_sequences(encoder_input, maxlen=max_src_len, padding='post')\n",
        "decoder_input = pad_sequences(decoder_input, maxlen=max_tar_len, padding='post')\n",
        "decoder_target = pad_sequences(decoder_target, maxlen=max_tar_len, padding='post')"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bvUMDLcIbzSj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 모든 값에 대해서 One_Hot_Encoder 진행\n",
        "\n",
        "encoder_input = to_categorical(encoder_input)\n",
        "decoder_input = to_categorical(decoder_input)\n",
        "decoder_target = to_categorical(decoder_target)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AeYkTtN-b-Rt",
        "colab_type": "text"
      },
      "source": [
        "## Seq2Seq로 훈련"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LrHdHsNTb5xM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "import numpy as np"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZSiTj_opcE-9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder_inputs = Input(shape=(None, src_vocab_size))\n",
        "encoder_lstm = LSTM(units=256, return_state=True)\n",
        "encoder_outputs, state_h, state_c = encoder_lstm(encoder_inputs)\n",
        "# encoder_outputs도 같이 리턴받기는 했지만 여기서는 필요없으므로 이 값은 버림.\n",
        "\n",
        "encoder_states = [state_h, state_c]\n",
        "# LSTM은 바닐라 RNN과는 달리 상태가 두 개. 바로 은닉 상태와 셀 상태."
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pVl4AsmacFgG",
        "colab_type": "text"
      },
      "source": [
        "LSTM의 hidden layers는 256. 인코더의 내부 상태를 decoder로 넘겨주어야 하기 때문에  \n",
        "return_state=True로 설정  \n",
        "LSTM에서 state_h, state_c를 return받는데, 각각 은닉 상태와 셀 상태에 해당된다.  \n",
        "이 두 가지 상태를 encoder_state에 저장한다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AZOcMzMVh9uf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "decoder_inputs = Input(shape=(None, tar_vocab_size))\n",
        "decoder_lstm = LSTM(units=256, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _= decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
        "# 디코더의 첫 상태를 인코더의 은닉 상태, 셀 상태로\n",
        "\n",
        "decoder_softmax_layer = Dense(tar_vocab_size, activation='softmax')\n",
        "decoder_outputs = decoder_softmax_layer(decoder_outputs)\n",
        "\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\")"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MCXU-H54h_U8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 691
        },
        "outputId": "1e09f621-445d-4379-f3c7-93634819677c"
      },
      "source": [
        "# 입력으로 인코더와 디코더가 들어가고, 디코더의 실제값인 decoder_target도 필요하다. 배치는 60으로, 학습은 20으로(시간 관계 상) 조정\n",
        "# 데이터가 과적합 된 상태로 진행\n",
        "\n",
        "model.fit(x=[encoder_input, decoder_input], y=decoder_target, batch_size=64, epochs=20, validation_split=0.2)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "750/750 [==============================] - 19s 25ms/step - loss: 0.3691 - val_loss: 0.3569\n",
            "Epoch 2/20\n",
            "750/750 [==============================] - 18s 24ms/step - loss: 0.3406 - val_loss: 0.3402\n",
            "Epoch 3/20\n",
            "750/750 [==============================] - 18s 24ms/step - loss: 0.3264 - val_loss: 0.3314\n",
            "Epoch 4/20\n",
            "750/750 [==============================] - 18s 24ms/step - loss: 0.3162 - val_loss: 0.3247\n",
            "Epoch 5/20\n",
            "750/750 [==============================] - 18s 24ms/step - loss: 0.3082 - val_loss: 0.3186\n",
            "Epoch 6/20\n",
            "750/750 [==============================] - 18s 24ms/step - loss: 0.3018 - val_loss: 0.3160\n",
            "Epoch 7/20\n",
            "750/750 [==============================] - 18s 24ms/step - loss: 0.2961 - val_loss: 0.3115\n",
            "Epoch 8/20\n",
            "750/750 [==============================] - 18s 24ms/step - loss: 0.2909 - val_loss: 0.3090\n",
            "Epoch 9/20\n",
            "750/750 [==============================] - 18s 24ms/step - loss: 0.2862 - val_loss: 0.3066\n",
            "Epoch 10/20\n",
            "750/750 [==============================] - 18s 24ms/step - loss: 0.2825 - val_loss: 0.3053\n",
            "Epoch 11/20\n",
            "750/750 [==============================] - 18s 24ms/step - loss: 0.2783 - val_loss: 0.3029\n",
            "Epoch 12/20\n",
            "750/750 [==============================] - 18s 24ms/step - loss: 0.2746 - val_loss: 0.3018\n",
            "Epoch 13/20\n",
            "750/750 [==============================] - 18s 24ms/step - loss: 0.2717 - val_loss: 0.3004\n",
            "Epoch 14/20\n",
            "750/750 [==============================] - 18s 24ms/step - loss: 0.2684 - val_loss: 0.3005\n",
            "Epoch 15/20\n",
            "750/750 [==============================] - 18s 24ms/step - loss: 0.2657 - val_loss: 0.2990\n",
            "Epoch 16/20\n",
            "750/750 [==============================] - 18s 24ms/step - loss: 0.2632 - val_loss: 0.2988\n",
            "Epoch 17/20\n",
            "750/750 [==============================] - 18s 24ms/step - loss: 0.2606 - val_loss: 0.2989\n",
            "Epoch 18/20\n",
            "750/750 [==============================] - 18s 24ms/step - loss: 0.2583 - val_loss: 0.2975\n",
            "Epoch 19/20\n",
            "750/750 [==============================] - 18s 24ms/step - loss: 0.2561 - val_loss: 0.2976\n",
            "Epoch 20/20\n",
            "750/750 [==============================] - 18s 24ms/step - loss: 0.2539 - val_loss: 0.2990\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f382c237ef0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9IcrCybHiDgM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1802e8a6-ed1a-4b47-92ee-2c246eaa0a2a"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, None, 103)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, None, 75)]   0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lstm (LSTM)                     [(None, 256), (None, 368640      input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   [(None, None, 256),  339968      input_2[0][0]                    \n",
            "                                                                 lstm[0][1]                       \n",
            "                                                                 lstm[0][2]                       \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, None, 75)     19275       lstm_1[0][0]                     \n",
            "==================================================================================================\n",
            "Total params: 727,883\n",
            "Trainable params: 727,883\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xAGN1Cd8j9z4",
        "colab_type": "text"
      },
      "source": [
        "Seq2Seq 기계 번역기 동작시키기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfREEqS2kIpC",
        "colab_type": "text"
      },
      "source": [
        "전체적인 번역 동작 단계를 정리하면  \n",
        "1. 번역하고자 하는 입력 문장이 인코더에 들어가서 은닉 상태와 셀 상태를 얻는다.  \n",
        "2. 상태와 <SOS>에 해당하는 '\\t'를 디코더로 보낸다.  \n",
        "3. 디코더가 <EOS>에 해당하는 '\\n'이 나올 때까지 다음 문자를 예측하는 행동을 반복한다.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MHCDbTtWkPV7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder_model = Model(inputs=encoder_inputs, outputs=encoder_states)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Olsu4tkPkWsP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 이전 시점의 상태들을 저장하는 텐서\n",
        "\n",
        "decoder_state_input_h = Input(shape=(256,))\n",
        "decoder_state_input_c = Input(shape=(256,))\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
        "\n",
        "# 문장의 다음 단어를 예측하기 위해서 초기 상태(initial_state)를 이전 시점의 상태로 사용. 이는 뒤의 함수 decode_sequence()에 구현\n",
        "decoder_states = [state_h, state_c]\n",
        "\n",
        "# 훈련 과정에서와 달리 LSTM의 리턴하는 은닉 상태와 셀 상태인 state_h와 state_c를 버리지 않음.\n",
        "decoder_outputs = decoder_softmax_layer(decoder_outputs)\n",
        "decoder_model = Model(inputs=[decoder_inputs] + decoder_states_inputs, outputs=[decoder_outputs] + decoder_states)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "afHdcArzkcLS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "index_to_src = dict((i, char) for char, i in src_to_index.items())\n",
        "index_to_tar = dict((i, char) for char, i in tar_to_index.items())"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xjfwg0hTkeEA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 단어로부터 인덱스를 얻는 것이 아니라, 인덱스로부터 단어를 얻을 수 있는 index_to_src와 index_to_tar를 만들었다.\n",
        "\n",
        "def decode_sequence(input_seq):\n",
        "    # 입력으로부터 인코더의 상태를 얻음\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "\n",
        "    # <SOS>에 해당하는 원-핫 벡터 생성\n",
        "    target_seq = np.zeros((1, 1, tar_vocab_size))\n",
        "    target_seq[0, 0, tar_to_index['\\t']] = 1.\n",
        "\n",
        "    stop_condition = False\n",
        "    decoded_sentence = \"\"\n",
        "\n",
        "    # stop_condition이 True가 될 때까지 루프 반복\n",
        "    while not stop_condition:\n",
        "        # 이점 시점의 상태 states_value를 현 시점의 초기 상태로 사용\n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
        "\n",
        "        # 예측 결과를 문자로 변환\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_char = index_to_tar[sampled_token_index]\n",
        "\n",
        "        # 현재 시점의 예측 문자를 예측 문장에 추가\n",
        "        decoded_sentence += sampled_char\n",
        "\n",
        "        # <eos>에 도달하거나 최대 길이를 넘으면 중단.\n",
        "        if (sampled_char == '\\n' or\n",
        "           len(decoded_sentence) > max_tar_len):\n",
        "            stop_condition = True\n",
        "\n",
        "        # 현재 시점의 예측 결과를 다음 시점의 입력으로 사용하기 위해 저장\n",
        "        target_seq = np.zeros((1, 1, tar_vocab_size))\n",
        "        target_seq[0, 0, sampled_token_index] = 1.\n",
        "\n",
        "        # 현재 시점의 상태를 다음 시점의 상태로 사용하기 위해 저장\n",
        "        states_value = [h, c]\n",
        "\n",
        "    return decoded_sentence"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f85XHUEvkmo0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "outputId": "8d452503-00b4-44a8-cf65-9d4b497d4411"
      },
      "source": [
        "for seq_index in [3,50,100,300,1001]: # 입력 문장의 인덱스\n",
        "    input_seq = encoder_input[seq_index: seq_index + 1]\n",
        "    decoded_sentence = decode_sequence(input_seq)\n",
        "    print(35 * \"-\")\n",
        "    print('입력 문장:', lines.src[seq_index])\n",
        "    print('정답 문장:', lines.tar[seq_index][1:len(lines.tar[seq_index])-1]) # '\\t'와 '\\n'을 빼고 출력\n",
        "    print('번역기가 번역한 문장:', decoded_sentence[:len(decoded_sentence)-1]) # '\\n'을 빼고 출력"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-----------------------------------\n",
            "입력 문장: Cours !\n",
            "정답 문장:  CC-BY 2.0 (France) Attribution: tatoeba.org #906328 (papabear) & #906331 (sacredceltic) \n",
            "번역기가 번역한 문장:  CC-BY 2.0 (France) Attribution: tatoeba.org #2249602 (CK) & #3647320 (sacredceltic) \n",
            "-----------------------------------\n",
            "입력 문장: J'ai menti.\n",
            "정답 문장:  CC-BY 2.0 (France) Attribution: tatoeba.org #3935374 (gianich73) & #7581897 (Micsmithel) \n",
            "번역기가 번역한 문장:  CC-BY 2.0 (France) Attribution: tatoeba.org #2247402 (CK) & #4681769 (sacredceltic) \n",
            "-----------------------------------\n",
            "입력 문장: Entre.\n",
            "정답 문장:  CC-BY 2.0 (France) Attribution: tatoeba.org #348091 (Hertz) & #585174 (sacredceltic) \n",
            "번역기가 번역한 문장:  CC-BY 2.0 (France) Attribution: tatoeba.org #2249502 (CK) & #3647327 (sacredceltic) \n",
            "-----------------------------------\n",
            "입력 문장: Magnez-vous !\n",
            "정답 문장:  CC-BY 2.0 (France) Attribution: tatoeba.org #1329 (brauliobezerra) & #8691461 (sacredceltic) \n",
            "번역기가 번역한 문장:  CC-BY 2.0 (France) Attribution: tatoeba.org #2249502 (CK) & #3647340 (sacredceltic) \n",
            "-----------------------------------\n",
            "입력 문장: Nous sommes allées à pied.\n",
            "정답 문장:  CC-BY 2.0 (France) Attribution: tatoeba.org #6790864 (shekitten) & #4947746 (sacredceltic) \n",
            "번역기가 번역한 문장:  CC-BY 2.0 (France) Attribution: tatoeba.org #2240626 (CK) & #4801779 (sacredceltic) \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pnzmVCe5kvDD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}